{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
   "metadata": {},
   "source": "# ADHAN Gemma 3 1B-it Training - LoRA Fine-tuning\n\n## Overview\nThis notebook fine-tunes Google's Gemma 3 1B Instruct model using LoRA (Low-Rank Adaptation)\non ADHAN's modern Tamil corpus with **sequential curriculum learning**.\n\n**Training Strategy**:\n- **Model**: `google/gemma-3-1b-it` (1B parameters, instruction-tuned)\n- **Method**: 4-bit LoRA fine-tuning (QLoRA)\n- **Corpus**: Modern Tamil Enhanced (train/val/test splits from `data/final/tamil_texts/hf/`)\n- **Learning**: Curriculum learning â€” **Phase 1** (modern sources) â†’ **Phase 2** (classical sources)\n- **Framework**: Hugging Face Transformers + PEFT\n\n**Benefits over XLM-RoBERTa**:\n1. Better Tamil Understanding: Gemma 3 has 2T token pretraining including Tamil\n2. Instruction-Following: Built-in chat format for Q&A tasks\n3. Mobile Deployment: Converts to GGUF (~600MB) for on-device inference\n4. Faster Training: 4-bit LoRA training completes in 30-45 minutes on single GPU\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-f6a7-8901-bcde-f12345678901",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-a7b8-9012-cdef-123456789012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "\n",
    "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PROJECT_ROOT = Path(os.environ.get('ADHAN_PROJECT_ROOT', Path.cwd().parent.parent))\n",
    "DATA_DIR     = PROJECT_ROOT / 'data' / 'final' / 'tamil_texts' / 'hf'\n",
    "MODELS_DIR   = PROJECT_ROOT / 'models' / 'adhan-gemma-v1'\n",
    "ADAPTER_DIR  = MODELS_DIR / 'lora_adapter'\n",
    "CKPT_DIR     = PROJECT_ROOT / 'models' / 'checkpoints' / 'gemma'\n",
    "\n",
    "for d in [MODELS_DIR, ADAPTER_DIR, CKPT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€ Key configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MODEL_NAME    = 'google/gemma-3-1b-it'\n",
    "MAX_LENGTH    = 512\n",
    "LORA_R        = 16\n",
    "LORA_ALPHA    = 32\n",
    "LORA_DROPOUT  = 0.05\n",
    "EPOCHS        = 3\n",
    "BATCH_SIZE    = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "print(f'PyTorch Version:       {torch.__version__}')\n",
    "print(f'Transformers Version:  {transformers.__version__}')\n",
    "print(f'CUDA Available:        {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU Device:            {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory:            {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "print(f'\\nProject root:  {PROJECT_ROOT}')\n",
    "print(f'Data dir:      {DATA_DIR}')\n",
    "print(f'Output dir:    {MODELS_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7-b8c9-0123-defa-234567890123",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8-c9d0-1234-efab-345678901234",
   "metadata": {},
   "outputs": [],
   "source": "def load_jsonl(path: Path) -> list[dict]:\n    \"\"\"Load a JSONL file and return a list of dicts.\"\"\"\n    records = []\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                records.append(json.loads(line))\n    return records\n\n\ndef resolve_val_path(data_dir: Path) -> Path:\n    \"\"\"Support both val.jsonl and validation.jsonl naming conventions.\"\"\"\n    for name in ('val.jsonl', 'validation.jsonl'):\n        p = data_dir / name\n        if p.exists():\n            return p\n    raise FileNotFoundError(f'No val/validation JSONL found in {data_dir}')\n\n\n# Source classification for curriculum learning\nMODERN_SOURCES    = {'social', 'news', 'modern_conversational', 'wikipedia', 'web'}\nCLASSICAL_SOURCES = {'thirukkural', 'sangam', 'classical', 'classical_poetry',\n                     'purananuru', 'akananuru', 'natrinai', 'kurunthokai'}\n\n\ndef classify_source(record: dict) -> str:\n    \"\"\"Return 'modern', 'classical', or 'other' for a data record.\"\"\"\n    src = record.get('source', '').lower()\n    if src in MODERN_SOURCES:\n        return 'modern'\n    if src in CLASSICAL_SOURCES:\n        return 'classical'\n    return 'other'\n\n\nprint('Loading JSONL splits...')\ntrain_records = load_jsonl(DATA_DIR / 'train.jsonl')\nval_records   = load_jsonl(resolve_val_path(DATA_DIR))\ntest_records  = load_jsonl(DATA_DIR / 'test.jsonl')\n\nprint(f'  Train:      {len(train_records):,} records')\nprint(f'  Validation: {len(val_records):,} records')\nprint(f'  Test:       {len(test_records):,} records')\n\n# Split training records by curriculum phase\nmodern_records    = [r for r in train_records if classify_source(r) == 'modern']\nclassical_records = [r for r in train_records if classify_source(r) == 'classical']\nother_records     = [r for r in train_records if classify_source(r) == 'other']\n\n# Records not explicitly classical participate in Phase 1\nphase1_records = modern_records + other_records\n\n# Phase 2 uses classical records; if none found, fall back to unclassified\n# (other_records) to avoid retraining on modern sources a second time.\nif classical_records:\n    phase2_records = classical_records\n    phase2_label   = 'classical'\nelif other_records:\n    import warnings\n    warnings.warn('No classical records found â€” Phase 2 will use unclassified records.')\n    phase2_records = other_records\n    phase2_label   = 'other (fallback â€” no classical found)'\nelse:\n    import warnings\n    warnings.warn('No classical or unclassified records â€” Phase 2 will reuse full training set.')\n    phase2_records = train_records\n    phase2_label   = 'full training set (fallback)'\n\nprint(f'\\nCurriculum split:')\nprint(f'  Phase 1 (modern + other):  {len(phase1_records):,} records')\nprint(f'  Phase 2 ({phase2_label}): {len(phase2_records):,} records')\nprint(f'\\nSample keys: {list(train_records[0].keys())}')\nprint(f'Sample text (first 200 chars):\\n  {train_records[0].get(\"text\", \"\")[:200]}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9-d0e1-2345-fabc-456789012345",
   "metadata": {},
   "outputs": [],
   "source": "def to_gemma_format(text: str) -> str:\n    \"\"\"\n    Wrap plain Tamil text in Gemma 3's instruction-following chat template.\n\n    The model is prompted to continue / explain the passage so that the\n    fine-tuning signal teaches it about Tamil language and culture.\n    \"\"\"\n    return (\n        '<start_of_turn>user\\n'\n        'à®¤à®®à®¿à®´à¯ à®‰à®°à¯ˆà®¯à¯ˆ à®¤à¯Šà®Ÿà®°à¯à®¨à¯à®¤à¯ à®à®´à¯à®¤à®µà¯à®®à¯:\\n'\n        f'{text}<end_of_turn>\\n'\n        '<start_of_turn>model\\n'\n        f'{text}<end_of_turn>'\n    )\n\n\ndef records_to_dataset(records: list[dict]) -> Dataset:\n    \"\"\"Convert a list of JSONL records to a HuggingFace Dataset.\"\"\"\n    texts = [to_gemma_format(r['text']) for r in records]\n    return Dataset.from_dict({'text': texts})\n\n\n# Full train / val / test for final evaluation\nraw_train  = records_to_dataset(train_records)\nraw_val    = records_to_dataset(val_records)\nraw_test   = records_to_dataset(test_records)\n\n# Phase-specific training sets for curriculum learning\nraw_phase1 = records_to_dataset(phase1_records)\nraw_phase2 = records_to_dataset(phase2_records)\n\nprint(f'Dataset sizes:')\nprint(f'  Full train:  {len(raw_train):,}')\nprint(f'  Phase 1:     {len(raw_phase1):,}')\nprint(f'  Phase 2:     {len(raw_phase2):,}')\nprint(f'\\nSample formatted entry:')\nprint(raw_train[0]['text'][:400])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0-e1f2-3456-abcd-567890123456",
   "metadata": {},
   "outputs": [],
   "source": "print(f'Loading tokenizer for {MODEL_NAME} ...')\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\n\n\ndef tokenize(batch):\n    return tokenizer(\n        batch['text'],\n        truncation=True,\n        max_length=MAX_LENGTH,\n        padding='max_length',\n    )\n\n\ndef prepare_dataset(raw: Dataset) -> Dataset:\n    \"\"\"Tokenize a raw text dataset and attach causal-LM labels.\"\"\"\n    ds = raw.map(tokenize, batched=True, remove_columns=['text'])\n    ds = ds.map(lambda ex: {'labels': ex['input_ids']})\n    ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    return ds\n\n\nprint('Tokenizing datasets...')\ntrain_dataset  = prepare_dataset(raw_train)\nval_dataset    = prepare_dataset(raw_val)\ntest_dataset   = prepare_dataset(raw_test)\nphase1_dataset = prepare_dataset(raw_phase1)\nphase2_dataset = prepare_dataset(raw_phase2)\n\nprint(f'\\nâœ… Tokenized datasets:')\nprint(f'   Train (full): {train_dataset}')\nprint(f'   Validation:   {val_dataset}')\nprint(f'   Test:         {test_dataset}')\nprint(f'   Phase 1:      {phase1_dataset}')\nprint(f'   Phase 2:      {phase2_dataset}')"
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1-f2a3-4567-bcde-678901234567",
   "metadata": {},
   "source": [
    "## 3. Load Model in 4-bit Quantization & Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2-a3b4-5678-cdef-789012345678",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Configuring 4-bit quantization (QLoRA)...')\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(f'Loading {MODEL_NAME} ...')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.config.use_cache = False  # Required for gradient checkpointing\n",
    "\n",
    "param_count = sum(p.numel() for p in model.parameters())\n",
    "print(f'\\nâœ… Model loaded: {param_count / 1e9:.2f}B parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3-b4c5-6789-defa-890123456789",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Applying LoRA adapter...')\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj'],\n",
    "    bias='none',\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4-c5d6-7890-efab-901234567890",
   "metadata": {},
   "source": "## 4. Sequential / Curriculum Training\n\nTraining proceeds in two sequential phases:\n\n| Phase | Data | Epochs | Learning Rate | Purpose |\n|-------|------|--------|---------------|---------|\n| 1 | Modern Tamil (social, news, web, wikipedia) | 2 | 2e-4 | Fluency & modern usage |\n| 2 | Classical Tamil (Thirukkural, Sangam, etc.) | 1 | 1e-4 | Classical literature |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5-d6e7-8901-fabc-012345678901",
   "metadata": {},
   "outputs": [],
   "source": "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\nPHASE1_EPOCHS = 2\nPHASE2_EPOCHS = 1\n\n# â”€â”€ Phase 1: Modern Tamil â€” learn fluency & contemporary usage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nphase1_args = TrainingArguments(\n    output_dir=str(CKPT_DIR / 'phase1'),\n    num_train_epochs=PHASE1_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    learning_rate=LEARNING_RATE,\n    fp16=True,\n    logging_steps=10,\n    eval_strategy='steps',\n    eval_steps=100,\n    save_steps=100,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    warmup_ratio=0.05,\n    lr_scheduler_type='cosine',\n    gradient_checkpointing=True,\n    report_to='none',\n    run_name=f'adhan-gemma-phase1-{datetime.now().strftime(\"%Y%m%d_%H%M\")}',\n)\n\nphase1_trainer = Trainer(\n    model=model,\n    args=phase1_args,\n    train_dataset=phase1_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n)\n\nprint('=' * 70)\nprint('PHASE 1 â€” Modern Tamil Sources')\nprint('=' * 70)\nprint(f'  Records:       {len(phase1_dataset):,}')\nprint(f'  Epochs:        {PHASE1_EPOCHS}')\nprint(f'  Learning rate: {LEARNING_RATE}')\n\nphase1_result = phase1_trainer.train()\nprint(f'\\nâœ… Phase 1 complete!')\nprint(f'   Loss:    {phase1_result.training_loss:.4f}')\nprint(f'   Runtime: {phase1_result.metrics[\"train_runtime\"]:.0f}s')\n\n# â”€â”€ Phase 2: Classical Tamil â€” refine on literary corpus â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nphase2_args = TrainingArguments(\n    output_dir=str(CKPT_DIR / 'phase2'),\n    num_train_epochs=PHASE2_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    # Lower LR preserves Phase 1 learning while fine-tuning on classical sources\n    learning_rate=LEARNING_RATE / 2,\n    fp16=True,\n    logging_steps=10,\n    eval_strategy='steps',\n    eval_steps=100,\n    save_steps=100,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    warmup_ratio=0.03,\n    lr_scheduler_type='cosine',\n    gradient_checkpointing=True,\n    report_to='none',\n    run_name=f'adhan-gemma-phase2-{datetime.now().strftime(\"%Y%m%d_%H%M\")}',\n)\n\nphase2_trainer = Trainer(\n    model=model,\n    args=phase2_args,\n    train_dataset=phase2_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n)\n\nprint('\\n' + '=' * 70)\nprint('PHASE 2 â€” Classical Tamil Sources')\nprint('=' * 70)\nprint(f'  Records:       {len(phase2_dataset):,}')\nprint(f'  Epochs:        {PHASE2_EPOCHS}')\nprint(f'  Learning rate: {LEARNING_RATE / 2}')\n\nphase2_result = phase2_trainer.train()\nprint(f'\\nâœ… Phase 2 complete!')\nprint(f'   Loss:    {phase2_result.training_loss:.4f}')\nprint(f'   Runtime: {phase2_result.metrics[\"train_runtime\"]:.0f}s')\n\n# Expose the final trainer for downstream evaluation / saving\ntrainer      = phase2_trainer\ntrain_result = phase2_result"
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6-e7f8-9012-abcd-123456789012",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7-f8a9-0123-bcde-234567890123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluating on validation set...')\n",
    "val_metrics = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(f'  Validation loss:     {val_metrics[\"eval_loss\"]:.4f}')\n",
    "\n",
    "print('\\nEvaluating on test set...')\n",
    "test_metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(f'  Test loss:           {test_metrics[\"eval_loss\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8-a9b0-1234-cdef-345678901234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tamil(prompt: str, max_new_tokens: int = 200) -> str:\n",
    "    \"\"\"Generate Tamil text from a given prompt using the fine-tuned model.\"\"\"\n",
    "    formatted = (\n",
    "        '<start_of_turn>user\\n'\n",
    "        f'{prompt}<end_of_turn>\\n'\n",
    "        '<start_of_turn>model\\n'\n",
    "    )\n",
    "    inputs = tokenizer(formatted, return_tensors='pt').to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Return only the model's response\n",
    "    if 'model\\n' in decoded:\n",
    "        return decoded.split('model\\n', 1)[-1].strip()\n",
    "    return decoded.strip()\n",
    "\n",
    "\n",
    "test_prompts = [\n",
    "    'à®¤à®¿à®°à¯à®•à¯à®•à¯à®±à®³à®¿à®²à¯ à®‰à®³à¯à®³ à®’à®°à¯ à®•à¯à®±à®³à¯ à®šà¯Šà®²à¯à®²à®µà¯à®®à¯',\n",
    "    'à®¤à®®à®¿à®´à¯ à®‡à®²à®•à¯à®•à®¿à®¯à®¤à¯à®¤à®¿à®©à¯ à®šà®¿à®±à®ªà¯à®ªà¯à®•à®³à¯ à®¯à®¾à®µà¯ˆ?',\n",
    "    'à®šà®™à¯à®• à®‡à®²à®•à¯à®•à®¿à®¯à®®à¯ à®ªà®±à¯à®±à®¿ à®šà¯à®°à¯à®•à¯à®•à®®à®¾à®• à®µà®¿à®³à®•à¯à®•à®µà¯à®®à¯',\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f'\\nğŸ“ Prompt: {prompt}')\n",
    "    response = generate_tamil(prompt)\n",
    "    print(f'ğŸ’¬ Response: {response[:300]}')\n",
    "\n",
    "    # Check Tamil character ratio\n",
    "    tamil_chars = sum(1 for c in response if '\\u0B80' <= c <= '\\u0BFF')\n",
    "    total_chars = max(sum(1 for c in response if c.strip()), 1)\n",
    "    tamil_ratio = tamil_chars / total_chars\n",
    "    print(f'ğŸ“Š Tamil character ratio: {tamil_ratio:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9-b0c1-2345-defa-456789012345",
   "metadata": {},
   "source": [
    "## 6. Save Model & Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0-c1d2-3456-efab-567890123456",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Saving LoRA adapter to {ADAPTER_DIR} ...')\n",
    "model.save_pretrained(str(ADAPTER_DIR))\n",
    "\n",
    "print(f'Saving tokenizer to {MODELS_DIR} ...')\n",
    "tokenizer.save_pretrained(str(MODELS_DIR))\n",
    "\n",
    "# Persist training configuration\n",
    "config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'lora_r': LORA_R,\n",
    "    'lora_alpha': LORA_ALPHA,\n",
    "    'lora_dropout': LORA_DROPOUT,\n",
    "    'epochs': EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'train_loss': train_result.training_loss,\n",
    "    'val_loss': val_metrics['eval_loss'],\n",
    "    'test_loss': test_metrics['eval_loss'],\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "}\n",
    "with open(MODELS_DIR / 'training_config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print('\\nâœ… Saved:')\n",
    "print(f'   LoRA adapter: {ADAPTER_DIR}')\n",
    "print(f'   Tokenizer:    {MODELS_DIR}')\n",
    "print(f'   Config:       {MODELS_DIR / \"training_config.json\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1-d2e3-4567-fabc-678901234567",
   "metadata": {},
   "source": [
    "## 7. GGUF Conversion Instructions\n",
    "\n",
    "After training, merge the LoRA adapter into the base model and convert to GGUF format\n",
    "for efficient on-device inference using llama.cpp.\n",
    "\n",
    "### Step 1 â€“ Merge LoRA adapter into the base model\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'google/gemma-3-1b-it',\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")\n",
    "merged_model = PeftModel.from_pretrained(base_model, 'models/adhan-gemma-v1/lora_adapter')\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "merged_model.save_pretrained('models/adhan-gemma-v1/merged')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('models/adhan-gemma-v1')\n",
    "tokenizer.save_pretrained('models/adhan-gemma-v1/merged')\n",
    "```\n",
    "\n",
    "### Step 2 â€“ Clone llama.cpp and install dependencies\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "cd llama.cpp\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Step 3 â€“ Convert to GGUF\n",
    "\n",
    "```bash\n",
    "python convert_hf_to_gguf.py \\\n",
    "    ../models/adhan-gemma-v1/merged \\\n",
    "    --outfile ../models/adhan-gemma-v1/adhan-gemma-q4_k_m.gguf \\\n",
    "    --outtype q4_k_m\n",
    "```\n",
    "\n",
    "This produces a **~600 MB** Q4_K_M quantized GGUF file suitable for:\n",
    "- Desktop inference via `llama.cpp`\n",
    "- Mobile deployment via `llama.rn` or `MLC LLM`\n",
    "- Server-side inference with `Ollama`\n",
    "\n",
    "### Step 4 â€“ Test with llama.cpp\n",
    "\n",
    "```bash\n",
    "cd llama.cpp\n",
    "make -j$(nproc)\n",
    "./main \\\n",
    "    -m ../models/adhan-gemma-v1/adhan-gemma-q4_k_m.gguf \\\n",
    "    -p '<start_of_turn>user\\nà®¤à®¿à®°à¯à®•à¯à®•à¯à®±à®³à®¿à®²à¯ à®‰à®³à¯à®³ à®’à®°à¯ à®•à¯à®±à®³à¯ à®šà¯Šà®²à¯à®²à®µà¯à®®à¯<end_of_turn>\\n<start_of_turn>model\\n' \\\n",
    "    -n 200\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}