{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
   "metadata": {},
   "source": [
    "# ADHAN Gemma 3 1B-it Training - LoRA Fine-tuning\n",
    "\n",
    "## Overview\n",
    "This notebook fine-tunes Google's Gemma 3 1B Instruct model using LoRA (Low-Rank Adaptation)\n",
    "on ADHAN's modern Tamil corpus.\n",
    "\n",
    "**Training Strategy**:\n",
    "- **Model**: `google/gemma-3-1b-it` (1B parameters, instruction-tuned)\n",
    "- **Method**: 4-bit LoRA fine-tuning (QLoRA)\n",
    "- **Corpus**: Modern Tamil Enhanced (train/val/test splits from `data/final/tamil_texts/hf/`)\n",
    "- **Framework**: Hugging Face Transformers + PEFT\n",
    "\n",
    "**Benefits over XLM-RoBERTa**:\n",
    "1. Better Tamil Understanding: Gemma 3 has 2T token pretraining including Tamil\n",
    "2. Instruction-Following: Built-in chat format for Q&A tasks\n",
    "3. Mobile Deployment: Converts to GGUF (~600MB) for on-device inference\n",
    "4. Faster Training: 4-bit LoRA training completes in 30-45 minutes on single GPU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-f6a7-8901-bcde-f12345678901",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-a7b8-9012-cdef-123456789012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "\n",
    "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PROJECT_ROOT = Path(os.environ.get('ADHAN_PROJECT_ROOT', Path.cwd().parent.parent))\n",
    "DATA_DIR     = PROJECT_ROOT / 'data' / 'final' / 'tamil_texts' / 'hf'\n",
    "MODELS_DIR   = PROJECT_ROOT / 'models' / 'adhan-gemma-v1'\n",
    "ADAPTER_DIR  = MODELS_DIR / 'lora_adapter'\n",
    "CKPT_DIR     = PROJECT_ROOT / 'models' / 'checkpoints' / 'gemma'\n",
    "\n",
    "for d in [MODELS_DIR, ADAPTER_DIR, CKPT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€ Key configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MODEL_NAME    = 'google/gemma-3-1b-it'\n",
    "MAX_LENGTH    = 512\n",
    "LORA_R        = 16\n",
    "LORA_ALPHA    = 32\n",
    "LORA_DROPOUT  = 0.05\n",
    "EPOCHS        = 3\n",
    "BATCH_SIZE    = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "print(f'PyTorch Version:       {torch.__version__}')\n",
    "print(f'Transformers Version:  {transformers.__version__}')\n",
    "print(f'CUDA Available:        {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU Device:            {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory:            {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "print(f'\\nProject root:  {PROJECT_ROOT}')\n",
    "print(f'Data dir:      {DATA_DIR}')\n",
    "print(f'Output dir:    {MODELS_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7-b8c9-0123-defa-234567890123",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8-c9d0-1234-efab-345678901234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path: Path) -> list[dict]:\n",
    "    \"\"\"Load a JSONL file and return a list of dicts.\"\"\"\n",
    "    records = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                records.append(json.loads(line))\n",
    "    return records\n",
    "\n",
    "\n",
    "print('Loading JSONL splits...')\n",
    "train_records = load_jsonl(DATA_DIR / 'train.jsonl')\n",
    "val_records   = load_jsonl(DATA_DIR / 'val.jsonl')\n",
    "test_records  = load_jsonl(DATA_DIR / 'test.jsonl')\n",
    "\n",
    "print(f'  Train:      {len(train_records):,} records')\n",
    "print(f'  Validation: {len(val_records):,} records')\n",
    "print(f'  Test:       {len(test_records):,} records')\n",
    "print(f'\\nSample keys: {list(train_records[0].keys())}')\n",
    "print(f'Sample text (first 200 chars):\\n  {train_records[0].get(\"text\", \"\")[:200]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9-d0e1-2345-fabc-456789012345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gemma_format(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Wrap plain Tamil text in Gemma 3's instruction-following chat template.\n",
    "\n",
    "    The model is prompted to continue / explain the passage so that the\n",
    "    fine-tuning signal teaches it about Tamil language and culture.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        '<start_of_turn>user\\n'\n",
    "        'à®¤à®®à®¿à®´à¯ à®‰à®°à¯ˆà®¯à¯ˆ à®¤à¯Šà®Ÿà®°à¯à®¨à¯à®¤à¯ à®à®´à¯à®¤à®µà¯à®®à¯:\\n'\n",
    "        f'{text}<end_of_turn>\\n'\n",
    "        '<start_of_turn>model\\n'\n",
    "        f'{text}<end_of_turn>'\n",
    "    )\n",
    "\n",
    "\n",
    "def records_to_dataset(records: list[dict]) -> Dataset:\n",
    "    \"\"\"Convert a list of JSONL records to a HuggingFace Dataset.\"\"\"\n",
    "    texts = [to_gemma_format(r['text']) for r in records]\n",
    "    return Dataset.from_dict({'text': texts})\n",
    "\n",
    "\n",
    "raw_train = records_to_dataset(train_records)\n",
    "raw_val   = records_to_dataset(val_records)\n",
    "raw_test  = records_to_dataset(test_records)\n",
    "\n",
    "print('Sample formatted entry:')\n",
    "print(raw_train[0]['text'][:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0-e1f2-3456-abcd-567890123456",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loading tokenizer for {MODEL_NAME} ...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "    )\n",
    "\n",
    "\n",
    "print('Tokenizing datasets...')\n",
    "train_dataset = raw_train.map(tokenize, batched=True, remove_columns=['text'])\n",
    "val_dataset   = raw_val.map(tokenize,   batched=True, remove_columns=['text'])\n",
    "test_dataset  = raw_test.map(tokenize,  batched=True, remove_columns=['text'])\n",
    "\n",
    "# Labels are input_ids (causal LM objective)\n",
    "train_dataset = train_dataset.map(lambda ex: {'labels': ex['input_ids']})\n",
    "val_dataset   = val_dataset.map(lambda ex: {'labels': ex['input_ids']})\n",
    "test_dataset  = test_dataset.map(lambda ex: {'labels': ex['input_ids']})\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch',   columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch',  columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "print(f'\\nâœ… Tokenized datasets:')\n",
    "print(f'   Train:      {train_dataset}')\n",
    "print(f'   Validation: {val_dataset}')\n",
    "print(f'   Test:       {test_dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1-f2a3-4567-bcde-678901234567",
   "metadata": {},
   "source": [
    "## 3. Load Model in 4-bit Quantization & Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2-a3b4-5678-cdef-789012345678",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Configuring 4-bit quantization (QLoRA)...')\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(f'Loading {MODEL_NAME} ...')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model.config.use_cache = False  # Required for gradient checkpointing\n",
    "\n",
    "param_count = sum(p.numel() for p in model.parameters())\n",
    "print(f'\\nâœ… Model loaded: {param_count / 1e9:.2f}B parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3-b4c5-6789-defa-890123456789",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Applying LoRA adapter...')\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj'],\n",
    "    bias='none',\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4-c5d6-7890-efab-901234567890",
   "metadata": {},
   "source": [
    "## 4. LoRA Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5-d6e7-8901-fabc-012345678901",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=str(CKPT_DIR),\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type='cosine',\n",
    "    gradient_checkpointing=True,\n",
    "    report_to='none',\n",
    "    run_name=f'adhan-gemma-{datetime.now().strftime(\"%Y%m%d_%H%M\")}',\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print('Starting LoRA fine-tuning...')\n",
    "print(f'  Epochs:        {EPOCHS}')\n",
    "print(f'  Batch size:    {BATCH_SIZE}')\n",
    "print(f'  Learning rate: {LEARNING_RATE}')\n",
    "print(f'  LoRA rank:     {LORA_R}')\n",
    "print(f'  LoRA alpha:    {LORA_ALPHA}')\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(f'\\nâœ… Training complete!')\n",
    "print(f'   Loss:          {train_result.training_loss:.4f}')\n",
    "print(f'   Runtime:       {train_result.metrics[\"train_runtime\"]:.0f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6-e7f8-9012-abcd-123456789012",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7-f8a9-0123-bcde-234567890123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Evaluating on validation set...')\n",
    "val_metrics = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(f'  Validation loss:     {val_metrics[\"eval_loss\"]:.4f}')\n",
    "\n",
    "print('\\nEvaluating on test set...')\n",
    "test_metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(f'  Test loss:           {test_metrics[\"eval_loss\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8-a9b0-1234-cdef-345678901234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tamil(prompt: str, max_new_tokens: int = 200) -> str:\n",
    "    \"\"\"Generate Tamil text from a given prompt using the fine-tuned model.\"\"\"\n",
    "    formatted = (\n",
    "        '<start_of_turn>user\\n'\n",
    "        f'{prompt}<end_of_turn>\\n'\n",
    "        '<start_of_turn>model\\n'\n",
    "    )\n",
    "    inputs = tokenizer(formatted, return_tensors='pt').to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Return only the model's response\n",
    "    if 'model\\n' in decoded:\n",
    "        return decoded.split('model\\n', 1)[-1].strip()\n",
    "    return decoded.strip()\n",
    "\n",
    "\n",
    "test_prompts = [\n",
    "    'à®¤à®¿à®°à¯à®•à¯à®•à¯à®±à®³à®¿à®²à¯ à®‰à®³à¯à®³ à®’à®°à¯ à®•à¯à®±à®³à¯ à®šà¯Šà®²à¯à®²à®µà¯à®®à¯',\n",
    "    'à®¤à®®à®¿à®´à¯ à®‡à®²à®•à¯à®•à®¿à®¯à®¤à¯à®¤à®¿à®©à¯ à®šà®¿à®±à®ªà¯à®ªà¯à®•à®³à¯ à®¯à®¾à®µà¯ˆ?',\n",
    "    'à®šà®™à¯à®• à®‡à®²à®•à¯à®•à®¿à®¯à®®à¯ à®ªà®±à¯à®±à®¿ à®šà¯à®°à¯à®•à¯à®•à®®à®¾à®• à®µà®¿à®³à®•à¯à®•à®µà¯à®®à¯',\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f'\\nğŸ“ Prompt: {prompt}')\n",
    "    response = generate_tamil(prompt)\n",
    "    print(f'ğŸ’¬ Response: {response[:300]}')\n",
    "\n",
    "    # Check Tamil character ratio\n",
    "    tamil_chars = sum(1 for c in response if '\\u0B80' <= c <= '\\u0BFF')\n",
    "    total_chars = max(sum(1 for c in response if c.strip()), 1)\n",
    "    tamil_ratio = tamil_chars / total_chars\n",
    "    print(f'ğŸ“Š Tamil character ratio: {tamil_ratio:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9-b0c1-2345-defa-456789012345",
   "metadata": {},
   "source": [
    "## 6. Save Model & Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0-c1d2-3456-efab-567890123456",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Saving LoRA adapter to {ADAPTER_DIR} ...')\n",
    "model.save_pretrained(str(ADAPTER_DIR))\n",
    "\n",
    "print(f'Saving tokenizer to {MODELS_DIR} ...')\n",
    "tokenizer.save_pretrained(str(MODELS_DIR))\n",
    "\n",
    "# Persist training configuration\n",
    "config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'lora_r': LORA_R,\n",
    "    'lora_alpha': LORA_ALPHA,\n",
    "    'lora_dropout': LORA_DROPOUT,\n",
    "    'epochs': EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'train_loss': train_result.training_loss,\n",
    "    'val_loss': val_metrics['eval_loss'],\n",
    "    'test_loss': test_metrics['eval_loss'],\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "}\n",
    "with open(MODELS_DIR / 'training_config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print('\\nâœ… Saved:')\n",
    "print(f'   LoRA adapter: {ADAPTER_DIR}')\n",
    "print(f'   Tokenizer:    {MODELS_DIR}')\n",
    "print(f'   Config:       {MODELS_DIR / \"training_config.json\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1-d2e3-4567-fabc-678901234567",
   "metadata": {},
   "source": [
    "## 7. GGUF Conversion Instructions\n",
    "\n",
    "After training, merge the LoRA adapter into the base model and convert to GGUF format\n",
    "for efficient on-device inference using llama.cpp.\n",
    "\n",
    "### Step 1 â€“ Merge LoRA adapter into the base model\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'google/gemma-3-1b-it',\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")\n",
    "merged_model = PeftModel.from_pretrained(base_model, 'models/adhan-gemma-v1/lora_adapter')\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "merged_model.save_pretrained('models/adhan-gemma-v1/merged')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('models/adhan-gemma-v1')\n",
    "tokenizer.save_pretrained('models/adhan-gemma-v1/merged')\n",
    "```\n",
    "\n",
    "### Step 2 â€“ Clone llama.cpp and install dependencies\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "cd llama.cpp\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Step 3 â€“ Convert to GGUF\n",
    "\n",
    "```bash\n",
    "python convert_hf_to_gguf.py \\\n",
    "    ../models/adhan-gemma-v1/merged \\\n",
    "    --outfile ../models/adhan-gemma-v1/adhan-gemma-q4_k_m.gguf \\\n",
    "    --outtype q4_k_m\n",
    "```\n",
    "\n",
    "This produces a **~600 MB** Q4_K_M quantized GGUF file suitable for:\n",
    "- Desktop inference via `llama.cpp`\n",
    "- Mobile deployment via `llama.rn` or `MLC LLM`\n",
    "- Server-side inference with `Ollama`\n",
    "\n",
    "### Step 4 â€“ Test with llama.cpp\n",
    "\n",
    "```bash\n",
    "cd llama.cpp\n",
    "make -j$(nproc)\n",
    "./main \\\n",
    "    -m ../models/adhan-gemma-v1/adhan-gemma-q4_k_m.gguf \\\n",
    "    -p '<start_of_turn>user\\nà®¤à®¿à®°à¯à®•à¯à®•à¯à®±à®³à®¿à®²à¯ à®‰à®³à¯à®³ à®’à®°à¯ à®•à¯à®±à®³à¯ à®šà¯Šà®²à¯à®²à®µà¯à®®à¯<end_of_turn>\\n<start_of_turn>model\\n' \\\n",
    "    -n 200\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
