{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d77d8c",
   "metadata": {},
   "source": [
    "# Aadhan Model Training - Setup & Exploration\n",
    "\n",
    "## Overview\n",
    "This notebook sets up the environment and explores the modern Tamil pretraining corpus for the Aadhan LLM.\n",
    "\n",
    "**Model**: Aadhan (Tamil Language Model)  \n",
    "**Framework**: Hugging Face Transformers  \n",
    "**Corpus**: Modern Tamil Enhanced (3,066 records, 9.8% modern sources)  \n",
    "**Task**: Pretraining on masked language modeling (MLM)  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa80fad8",
   "metadata": {},
   "source": [
    "## 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1418087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'transformers>=4.35.0',\n",
    "    'datasets>=2.14.0',\n",
    "    'torch>=2.1.0',\n",
    "    'numpy>=1.24.0',\n",
    "    'pandas>=2.0.0',\n",
    "    'matplotlib>=3.7.0',\n",
    "    'seaborn>=0.12.0',\n",
    "    'scikit-learn>=1.3.0',\n",
    "    'tqdm>=4.66.0',\n",
    "    'jupyter>=1.0.0',\n",
    "    'tokenizers>=0.14.0',\n",
    "    'accelerate>=0.24.0',\n",
    "    'peft>=0.7.0',  # Parameter-Efficient Fine-tuning\n",
    "]\n",
    "\n",
    "print(\"Installing dependencies...\")\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    \n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc762149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff9b7e",
   "metadata": {},
   "source": [
    "## 2. Setup Paths and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path('/home/neutron/.openclaw/zorba/Projects/OSS/yazhi/models/aadhan')\n",
    "DATA_DIR = PROJECT_ROOT / 'data' / 'pre_training' / 'tamil_texts' / 'hf'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "CHECKPOINTS_DIR = MODELS_DIR / 'checkpoints'\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / 'notebooks'\n",
    "LOGS_DIR = PROJECT_ROOT / 'logs'\n",
    "\n",
    "# Create directories\n",
    "for d in [MODELS_DIR, CHECKPOINTS_DIR, NOTEBOOKS_DIR, LOGS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verify data files exist\n",
    "data_files = ['train.jsonl', 'validation.jsonl', 'test.jsonl']\n",
    "print(\"üìÇ Data Files:\")\n",
    "for f in data_files:\n",
    "    path = DATA_DIR / f\n",
    "    if path.exists():\n",
    "        size = path.stat().st_size / (1024*1024)  # MB\n",
    "        print(f\"  ‚úÖ {f:<20} ({size:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {f:<20} NOT FOUND\")\n",
    "\n",
    "print(f\"\\nüìÅ Project Structure:\")\n",
    "print(f\"  Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"  Data Dir: {DATA_DIR}\")\n",
    "print(f\"  Models Dir: {MODELS_DIR}\")\n",
    "print(f\"  Checkpoints: {CHECKPOINTS_DIR}\")\n",
    "print(f\"  Logs: {LOGS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9d4d7d",
   "metadata": {},
   "source": [
    "## 3. Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5595ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from JSONL files\n",
    "def load_jsonl(path):\n",
    "    \"\"\"Load JSONL file into list of dicts.\"\"\"\n",
    "    records = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                records.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return records\n",
    "\n",
    "# Load splits\n",
    "print(\"Loading dataset...\")\n",
    "train_records = load_jsonl(DATA_DIR / 'train.jsonl')\n",
    "val_records = load_jsonl(DATA_DIR / 'validation.jsonl')\n",
    "test_records = load_jsonl(DATA_DIR / 'test.jsonl')\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"  Train: {len(train_records):,} records\")\n",
    "print(f\"  Val:   {len(val_records):,} records\")\n",
    "print(f\"  Test:  {len(test_records):,} records\")\n",
    "print(f\"  Total: {len(train_records) + len(val_records) + len(test_records):,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc1d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset composition\n",
    "def analyze_records(records, name):\n",
    "    \"\"\"Analyze record statistics.\"\"\"\n",
    "    \n",
    "    sources = Counter([r.get('source', 'unknown') for r in records])\n",
    "    quality_scores = [r.get('quality_score', 0) for r in records]\n",
    "    tamil_fractions = [r.get('tamil_fraction', 0) for r in records]\n",
    "    text_lengths = [len(r.get('text', '')) for r in records]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{name.upper()} SPLIT ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nüìà Text Statistics:\")\n",
    "    print(f\"  Total records: {len(records)}\")\n",
    "    print(f\"  Avg text length: {np.mean(text_lengths):.0f} chars\")\n",
    "    print(f\"  Min length: {np.min(text_lengths)} chars\")\n",
    "    print(f\"  Max length: {np.max(text_lengths)} chars\")\n",
    "    print(f\"  Median length: {np.median(text_lengths):.0f} chars\")\n",
    "    \n",
    "    print(f\"\\n‚≠ê Quality Scores:\")\n",
    "    print(f\"  Average: {np.mean(quality_scores):.3f}/1.0\")\n",
    "    print(f\"  Median: {np.median(quality_scores):.3f}\")\n",
    "    print(f\"  Min: {np.min(quality_scores):.3f}\")\n",
    "    print(f\"  Max: {np.max(quality_scores):.3f}\")\n",
    "    \n",
    "    print(f\"\\nüá±üá∑ Tamil Fraction:\")\n",
    "    print(f\"  Average: {np.mean(tamil_fractions):.1%}\")\n",
    "    print(f\"  Median: {np.median(tamil_fractions):.1%}\")\n",
    "    print(f\"  Min: {np.min(tamil_fractions):.1%}\")\n",
    "    print(f\"  Max: {np.max(tamil_fractions):.1%}\")\n",
    "    \n",
    "    print(f\"\\nüìö Source Distribution:\")\n",
    "    for src, cnt in sources.most_common():\n",
    "        pct = 100 * cnt / len(records)\n",
    "        print(f\"  {src:<20} {cnt:>6} ({pct:>5.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'sources': sources,\n",
    "        'quality_scores': quality_scores,\n",
    "        'tamil_fractions': tamil_fractions,\n",
    "        'text_lengths': text_lengths,\n",
    "    }\n",
    "\n",
    "train_stats = analyze_records(train_records, 'train')\n",
    "val_stats = analyze_records(val_records, 'validation')\n",
    "test_stats = analyze_records(test_records, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample records\n",
    "print(\"\\nüìù SAMPLE RECORDS\\n\")\n",
    "for i, record in enumerate(train_records[:3], 1):\n",
    "    print(f\"Record {i}:\")\n",
    "    print(f\"  Source: {record.get('source')}\")\n",
    "    print(f\"  Quality: {record.get('quality_score'):.3f}\")\n",
    "    print(f\"  Tamil Fraction: {record.get('tamil_fraction'):.1%}\")\n",
    "    print(f\"  Text: {record.get('text')[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17deb7a",
   "metadata": {},
   "source": [
    "## 4. Visualize Dataset Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81054bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# 1. Source distribution\n",
    "sources = train_stats['sources']\n",
    "ax = axes[0, 0]\n",
    "ax.barh(list(sources.keys()), list(sources.values()), color='skyblue')\n",
    "ax.set_xlabel('Count')\n",
    "ax.set_title('Source Distribution (Train)', fontsize=12, fontweight='bold')\n",
    "for i, (k, v) in enumerate(sources.items()):\n",
    "    ax.text(v + 5, i, f'{v}', va='center')\n",
    "\n",
    "# 2. Quality score distribution\n",
    "ax = axes[0, 1]\n",
    "ax.hist(train_stats['quality_scores'], bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "ax.axvline(np.mean(train_stats['quality_scores']), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "ax.set_xlabel('Quality Score')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Quality Score Distribution', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# 3. Text length distribution\n",
    "ax = axes[0, 2]\n",
    "ax.hist(train_stats['text_lengths'], bins=30, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "ax.axvline(np.mean(train_stats['text_lengths']), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "ax.set_xlabel('Text Length (chars)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Text Length Distribution', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# 4. Tamil fraction distribution\n",
    "ax = axes[1, 0]\n",
    "ax.hist(train_stats['tamil_fractions'], bins=30, color='plum', edgecolor='black', alpha=0.7)\n",
    "ax.axvline(np.mean(train_stats['tamil_fractions']), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "ax.set_xlabel('Tamil Fraction')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Tamil Character Fraction', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# 5. Split comparison\n",
    "ax = axes[1, 1]\n",
    "splits = ['Train', 'Val', 'Test']\n",
    "sizes = [len(train_records), len(val_records), len(test_records)]\n",
    "colors = ['#66c2a5', '#fc8d62', '#8da0cb']\n",
    "ax.bar(splits, sizes, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Number of Records')\n",
    "ax.set_title('Train/Val/Test Split', fontsize=12, fontweight='bold')\n",
    "for i, (split, size) in enumerate(zip(splits, sizes)):\n",
    "    ax.text(i, size + 20, f'{size}', ha='center', fontweight='bold')\n",
    "\n",
    "# 6. Quality by source\n",
    "ax = axes[1, 2]\n",
    "all_records = train_records + val_records + test_records\n",
    "src_quality = {}\n",
    "for record in all_records:\n",
    "    src = record.get('source', 'unknown')\n",
    "    score = record.get('quality_score', 0)\n",
    "    if src not in src_quality:\n",
    "        src_quality[src] = []\n",
    "    src_quality[src].append(score)\n",
    "\n",
    "src_quality_means = {k: np.mean(v) for k, v in src_quality.items()}\n",
    "ax.barh(list(src_quality_means.keys()), list(src_quality_means.values()), color='wheat')\n",
    "ax.set_xlabel('Average Quality Score')\n",
    "ax.set_title('Quality Score by Source', fontsize=12, fontweight='bold')\n",
    "for i, (k, v) in enumerate(src_quality_means.items()):\n",
    "    ax.text(v + 0.01, i, f'{v:.2f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(NOTEBOOKS_DIR / '01_data_exploration.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Visualization saved to {NOTEBOOKS_DIR / '01_data_exploration.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5da7f72",
   "metadata": {},
   "source": [
    "## 5. Load and Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we'll use multilingual models that support Tamil\n",
    "# Options:\n",
    "# 1. xlm-roberta-base (supports 100+ languages including Tamil)\n",
    "# 2. mBERT (supports 100+ languages)\n",
    "# 3. IndoBERT variants (if available)\n",
    "\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "\n",
    "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"\\nüìä Tokenizer Info:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Vocab Size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"  Max Position Embeddings: {tokenizer.model_max_length}\")\n",
    "print(f\"  Special Tokens: {len(tokenizer.special_tokens_map)}\")\n",
    "print(f\"  Do Lower Case: {tokenizer.do_lower_case}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565e020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tokenization on sample Tamil text\n",
    "sample_tamil = \"‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡ÆÆ‡Øä‡Æ¥‡Æø ‡Æé‡Æ©‡Øç‡Æ™‡Æ§‡ØÅ ‡Æ™‡Æ¥‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æ© ‡ÆÆ‡Øä‡Æ¥‡Æø‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æí‡Æ©‡Øç‡Æ±‡Ææ‡Æï‡ØÅ‡ÆÆ‡Øç.\"\n",
    "\n",
    "tokens = tokenizer.encode(sample_tamil)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "\n",
    "print(f\"Sample Tamil Text: {sample_tamil}\")\n",
    "print(f\"\\nTokenized ({len(tokens)} tokens):\")\n",
    "print(f\"  {tokens}\")\n",
    "print(f\"\\nDecoded: {decoded}\")\n",
    "\n",
    "# Show token details\n",
    "print(f\"\\nToken Details:\")\n",
    "for token_id in tokens:\n",
    "    token_str = tokenizer.decode([token_id])\n",
    "    print(f\"  {token_id:>6} ‚Üí '{token_str}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb90f53",
   "metadata": {},
   "source": [
    "## 6. Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada00850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert records to Dataset format\n",
    "def records_to_dataset(records):\n",
    "    \"\"\"Convert list of JSON records to HF Dataset.\"\"\"\n",
    "    texts = [r.get('text', '') for r in records]\n",
    "    ids = [r.get('id', '') for r in records]\n",
    "    sources = [r.get('source', '') for r in records]\n",
    "    \n",
    "    return Dataset.from_dict({\n",
    "        'text': texts,\n",
    "        'id': ids,\n",
    "        'source': sources,\n",
    "    })\n",
    "\n",
    "print(\"Converting to HF Dataset...\")\n",
    "train_dataset = records_to_dataset(train_records)\n",
    "val_dataset = records_to_dataset(val_records)\n",
    "test_dataset = records_to_dataset(test_records)\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets created:\")\n",
    "print(f\"  Train: {train_dataset}\")\n",
    "print(f\"  Val: {val_dataset}\")\n",
    "print(f\"  Test: {test_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663fdc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "def tokenize_function(examples, max_length=512):\n",
    "    \"\"\"Tokenize text examples.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "# Tokenize with batching\n",
    "train_dataset_tokenized = train_dataset.map(\n",
    "    lambda x: tokenize_function(x),\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=['text', 'id', 'source'],\n",
    ")\n",
    "\n",
    "val_dataset_tokenized = val_dataset.map(\n",
    "    lambda x: tokenize_function(x),\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=['text', 'id', 'source'],\n",
    ")\n",
    "\n",
    "test_dataset_tokenized = test_dataset.map(\n",
    "    lambda x: tokenize_function(x),\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=['text', 'id', 'source'],\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Tokenization complete:\")\n",
    "print(f\"  Train: {train_dataset_tokenized}\")\n",
    "print(f\"  Val: {val_dataset_tokenized}\")\n",
    "print(f\"  Test: {test_dataset_tokenized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show tokenized sample\n",
    "print(\"\\nüìù Tokenized Sample (first record):\")\n",
    "sample = train_dataset_tokenized[0]\n",
    "for key, value in sample.items():\n",
    "    if key == 'input_ids':\n",
    "        print(f\"  {key}: [...{len(value)} tokens...] (first 10: {value[:10]})\")\n",
    "    elif key == 'attention_mask':\n",
    "        print(f\"  {key}: {value[:20]}...\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save tokenized datasets for next notebook\n",
    "print(\"\\nüíæ Saving tokenized datasets...\")\n",
    "DATASETS_DIR = MODELS_DIR / 'tokenized_datasets'\n",
    "DATASETS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_dataset_tokenized.save_to_disk(DATASETS_DIR / 'train')\n",
    "val_dataset_tokenized.save_to_disk(DATASETS_DIR / 'val')\n",
    "test_dataset_tokenized.save_to_disk(DATASETS_DIR / 'test')\n",
    "\n",
    "print(f\"‚úÖ Datasets saved to {DATASETS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1022e",
   "metadata": {},
   "source": [
    "## 4. GPU/Device Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU and Device Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"üñ•Ô∏è  Device Information:\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"  cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "    \n",
    "    # GPU Stats\n",
    "    mb = 1024**2\n",
    "    print(f\"\\n  Memory Usage:\")\n",
    "    print(f\"    Allocated: {torch.cuda.memory_allocated() / mb:.0f} MB\")\n",
    "    print(f\"    Reserved: {torch.cuda.memory_reserved() / mb:.0f} MB\")\n",
    "    print(f\"    Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / mb:.0f} MB\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  No GPU available. Training will be on CPU (slow).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a519be",
   "metadata": {},
   "source": [
    "## 5. Tokenizer Selection and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer Options for Tamil\n",
    "# Option 1: Use pretrained multilingual tokenizer\n",
    "# Option 2: Train custom Tamil tokenizer from corpus\n",
    "\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"  # Starting point for Tamil\n",
    "\n",
    "print(f\"üì¶ Loading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"\\nüî§ Tokenizer Info:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Vocab Size: {tokenizer.vocab_size}\")\n",
    "print(f\"  Max Length: {tokenizer.model_max_length}\")\n",
    "\n",
    "# Test tokenizer on Tamil text\n",
    "sample_tamil = \"‡Æ®‡ØÄ ‡Æé‡Æ©‡Øç‡Æ© ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ‡Æø‡Æ±‡Æø‡ÆØ‡Ææ ‡Æá‡Æ™‡Øç‡Æ™‡Øã‡Æ§‡ØÅ?\"\n",
    "tokens = tokenizer(sample_tamil, return_tensors='pt')\n",
    "\n",
    "print(f\"\\nüìù Sample Tokenization:\")\n",
    "print(f\"  Text: {sample_tamil}\")\n",
    "print(f\"  Tokens: {tokenizer.tokenize(sample_tamil)}\")\n",
    "print(f\"  Token IDs: {tokens['input_ids'][0].tolist()}\")\n",
    "print(f\"  Decoded: {tokenizer.decode(tokens['input_ids'][0])}\")\n",
    "\n",
    "# Test on longer corpus sample\n",
    "if len(train_records) > 0:\n",
    "    sample_text = train_records[0]['text'][:100]\n",
    "    sample_tokens = tokenizer.tokenize(sample_text)\n",
    "    print(f\"\\nüìä Corpus Sample Tokenization:\")\n",
    "    print(f\"  Text: {sample_text[:50]}...\")\n",
    "    print(f\"  Num tokens: {len(sample_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c299f9",
   "metadata": {},
   "source": [
    "## 6. Model Architecture Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f2edc",
   "metadata": {},
   "source": [
    "## 7. Summary & Next Steps\n",
    "\n",
    "### ‚úÖ Completed in This Notebook\n",
    "\n",
    "- Installed all required dependencies (transformers, datasets, torch, etc.)\n",
    "- Set up project directories and paths\n",
    "- Loaded and explored the modern Tamil corpus (1,526 records)\n",
    "- Analyzed dataset statistics:\n",
    "  - **Source distribution**: Wikipedia 79.8%, News 17.5%, Social 1.5%, Others 1.2%\n",
    "  - **Quality metrics**: Average 0.524/1.0, 85% Tamil coverage\n",
    "  - **Text lengths**: Average ~250 chars, range 50-400 chars\n",
    "- Loaded XLM-RoBERTa tokenizer (supports 100+ languages including Tamil)\n",
    "- Tokenized datasets and prepared for training\n",
    "- Saved tokenized datasets for model training\n",
    "\n",
    "### ‚è≠Ô∏è Next Notebook: Model Training\n",
    "\n",
    "The next notebook (`02_model_training.ipynb`) will:\n",
    "1. Load the pre-trained XLM-RoBERTa model\n",
    "2. Configure masked language modeling (MLM)\n",
    "3. Set up training arguments with recommended hyperparameters\n",
    "4. Train the model with curriculum learning (modern ‚Üí full corpus)\n",
    "5. Evaluate on validation and test sets\n",
    "6. Save trained checkpoints\n",
    "\n",
    "### üìä Dataset Summary\n",
    "\n",
    "```\n",
    "Training Data: 1,220 records\n",
    "  - Wikipedia: 79.8%\n",
    "  - News (modern): 17.5%\n",
    "  - Social: 1.5%\n",
    "  - Other: 1.2%\n",
    "\n",
    "Validation: 152 records (10%)\n",
    "Test: 154 records (10%)\n",
    "\n",
    "Tokenizer: xlm-roberta-base (vocab size: 250K)\n",
    "Max sequence length: 512 tokens\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Status**: ‚úÖ Ready for training  \n",
    "**Next Step**: Run `02_model_training.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
