{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e186ab66",
   "metadata": {},
   "source": [
    "# Aadhan Model Training - Pretraining with MLM\n",
    "\n",
    "## Overview\n",
    "This notebook trains the Aadhan model using masked language modeling (MLM) on the modern Tamil corpus.\n",
    "\n",
    "**Training Strategy**:\n",
    "- **Model**: XLM-RoBERTa-base (pretrained, 124M parameters)\n",
    "- **Task**: Masked Language Modeling (MLM) - predict masked tokens\n",
    "- **Corpus**: Modern Tamil Enhanced (1,220 train, 152 val, 154 test records)\n",
    "- **Learning**: Curriculum learning (modern â†’ classical sources)\n",
    "- **Framework**: Hugging Face Transformers + Trainer API\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01788de",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Pre-tokenized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f048d0c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python 3.11 from \"/home/neutron/.openclaw/zorba/Projects/OSS/yazhi/models/aadhan/.venv/bin/python\"\n  * The NumPy version is: \"2.4.2\"\n\nand make sure that they are the versions you expect.\n\nPlease carefully study the information and documentation linked above.\nThis is unlikely to be a NumPy issue but will be caused by a bad install\nor environment on your machine.\n\nOriginal error was: libopenblas.so.0: cannot open shared object file: No such file or directory\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.openclaw/zorba/Projects/OSS/yazhi/models/aadhan/.venv/lib/python3.11/site-packages/numpy/_core/__init__.py:24\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multiarray\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.openclaw/zorba/Projects/OSS/yazhi/models/aadhan/.venv/lib/python3.11/site-packages/numpy/_core/multiarray.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctools\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _multiarray_umath, overrides\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_multiarray_umath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: libopenblas.so.0: cannot open shared object file: No such file or directory",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.openclaw/zorba/Projects/OSS/yazhi/models/aadhan/.venv/lib/python3.11/site-packages/numpy/__init__.py:112\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _distributor_init\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__config__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_config\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m e.name == \u001b[33m\"\u001b[39m\u001b[33mnumpy.__config__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    115\u001b[39m         \u001b[38;5;66;03m# The __config__ module itself was not found, so add this info:\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.openclaw/zorba/Projects/OSS/yazhi/models/aadhan/.venv/lib/python3.11/site-packages/numpy/__config__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This file is generated by numpy's build process\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# It contains system_info results at the time of building this package.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_multiarray_umath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     __cpu_features__,\n\u001b[32m      6\u001b[39m     __cpu_baseline__,\n\u001b[32m      7\u001b[39m     __cpu_dispatch__,\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mshow_config\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     11\u001b[39m _built_with_meson = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.openclaw/zorba/Projects/OSS/yazhi/models/aadhan/.venv/lib/python3.11/site-packages/numpy/_core/__init__.py:85\u001b[39m\n\u001b[32m     58\u001b[39m     major, minor, *_ = sys.version_info\n\u001b[32m     59\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[33mIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m \u001b[33mOriginal error was: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m envkey \u001b[38;5;129;01min\u001b[39;00m env_added:\n",
      "\u001b[31mImportError\u001b[39m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python 3.11 from \"/home/neutron/.openclaw/zorba/Projects/OSS/yazhi/models/aadhan/.venv/bin/python\"\n  * The NumPy version is: \"2.4.2\"\n\nand make sure that they are the versions you expect.\n\nPlease carefully study the information and documentation linked above.\nThis is unlikely to be a NumPy issue but will be caused by a bad install\nor environment on your machine.\n\nOriginal error was: libopenblas.so.0: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path('/home/neutron/.openclaw/zorba/Projects/OSS/yazhi/models/aadhan')\n",
    "DATASETS_DIR = PROJECT_ROOT / 'models' / 'tokenized_datasets'\n",
    "CHECKPOINTS_DIR = PROJECT_ROOT / 'models' / 'checkpoints'\n",
    "LOGS_DIR = PROJECT_ROOT / 'logs'\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / 'notebooks'\n",
    "\n",
    "# Create directories\n",
    "for d in [CHECKPOINTS_DIR, LOGS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a983a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-tokenized datasets\n",
    "print(\"Loading tokenized datasets...\")\n",
    "train_dataset = load_from_disk(DATASETS_DIR / 'train')\n",
    "val_dataset = load_from_disk(DATASETS_DIR / 'val')\n",
    "test_dataset = load_from_disk(DATASETS_DIR / 'test')\n",
    "\n",
    "print(f\"\\nâœ… Datasets loaded:\")\n",
    "print(f\"  Train: {train_dataset}\")\n",
    "print(f\"  Val: {val_dataset}\")\n",
    "print(f\"  Test: {test_dataset}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nğŸ“ Sample tokenized record:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"  Input IDs shape: {len(sample['input_ids'])}\")\n",
    "print(f\"  Attention Mask: {sum(sample['attention_mask'])} non-padding tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d4643",
   "metadata": {},
   "source": [
    "## 2. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8879fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "print(f\"\\nğŸ“Š Model Info:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"  Vocab Size: {model.config.vocab_size:,}\")\n",
    "print(f\"  Hidden Size: {model.config.hidden_size}\")\n",
    "print(f\"  Num Attention Heads: {model.config.num_attention_heads}\")\n",
    "print(f\"  Num Hidden Layers: {model.config.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8386c",
   "metadata": {},
   "source": [
    "## 3. Create Data Collator for MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39684d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for MLM - randomly masks tokens during training\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,  # Enable masked language modeling\n",
    "    mlm_probability=0.15,  # 15% of tokens will be masked (BERT standard)\n",
    ")\n",
    "\n",
    "print(\"âœ… Data Collator created for MLM training\")\n",
    "print(f\"  MLM Probability: 15%\")\n",
    "print(f\"  Masking Strategy:\")\n",
    "print(f\"    - 80% replaced with [MASK]\")\n",
    "print(f\"    - 10% replaced with random token\")\n",
    "print(f\"    - 10% unchanged (helps model learn token context)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a7c0e",
   "metadata": {},
   "source": [
    "## 4. Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455bf01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    # Output and checkpointing\n",
    "    output_dir=str(CHECKPOINTS_DIR),\n",
    "    save_strategy=\"epoch\",  # Save after each epoch\n",
    "    save_total_limit=3,  # Keep only 3 best checkpoints\n",
    "    logging_dir=str(LOGS_DIR),\n",
    "    logging_steps=50,\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=10,  # Train for 10 epochs\n",
    "    per_device_train_batch_size=32,  # Batch size per GPU\n",
    "    per_device_eval_batch_size=64,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=1,  # No gradient accumulation\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    learning_rate=5e-5,  # Standard BERT learning rate\n",
    "    warmup_steps=100,  # Linear warmup for 100 steps\n",
    "    weight_decay=0.01,  # L2 regularization\n",
    "    \n",
    "    # Evaluation and early stopping\n",
    "    eval_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",  # Use validation loss for early stopping\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"adamw_torch\",  # AdamW optimizer\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    "    # Mixed precision training (saves memory)\n",
    "    fp16=torch.cuda.is_available(),  # Use FP16 if GPU available\n",
    "    \n",
    "    # Misc\n",
    "    seed=42,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"âœ… Training Configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch Size: {training_args.per_device_train_batch_size} per device\")\n",
    "print(f\"  Learning Rate: {training_args.learning_rate}\")\n",
    "print(f\"  Warmup Steps: {training_args.warmup_steps}\")\n",
    "print(f\"  Weight Decay: {training_args.weight_decay}\")\n",
    "print(f\"  Mixed Precision: {training_args.fp16}\")\n",
    "print(f\"  Optimizer: {training_args.optim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba376a5",
   "metadata": {},
   "source": [
    "## 5. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4180f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer initialized\")\n",
    "print(f\"\\nğŸ“Š Training Dataset:\")\n",
    "print(f\"  Records: {len(train_dataset):,}\")\n",
    "print(f\"  Sequence Length: 512 tokens\")\n",
    "print(f\"  Estimated steps per epoch: {len(train_dataset) // training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Total training steps: {len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Validation Dataset:\")\n",
    "print(f\"  Records: {len(val_dataset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cbe267",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING MODEL TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Train\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Final Training Loss: {train_result.training_loss:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ac9795",
   "metadata": {},
   "source": [
    "## 7. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223187a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating on test set...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m test_results = \u001b[43mtrainer\u001b[49m.evaluate(eval_dataset=test_dataset)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ“Š Test Results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_results[\u001b[33m'\u001b[39m\u001b[33meval_loss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "print(f\"\\nğŸ“Š Test Results:\")\n",
    "print(f\"  Loss: {test_results['eval_loss']:.4f}\")\n",
    "print(f\"  Perplexity: {np.exp(test_results['eval_loss']):.2f}\")\n",
    "\n",
    "# Also evaluate on validation set for comparison\n",
    "print(f\"\\nğŸ“Š Validation Results (best checkpoint):\")\n",
    "val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(f\"  Loss: {val_results['eval_loss']:.4f}\")\n",
    "print(f\"  Perplexity: {np.exp(val_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a0ece",
   "metadata": {},
   "source": [
    "## 8. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fbfd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "FINAL_MODEL_DIR = PROJECT_ROOT / 'models' / 'aadhan-mlm-v1'\n",
    "FINAL_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Saving model to {FINAL_MODEL_DIR}...\")\n",
    "trainer.save_model(str(FINAL_MODEL_DIR))\n",
    "tokenizer.save_pretrained(str(FINAL_MODEL_DIR))\n",
    "\n",
    "# Save training results\n",
    "results_summary = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'final_model_dir': str(FINAL_MODEL_DIR),\n",
    "    'training_started': datetime.now().isoformat(),\n",
    "    'training_loss': float(train_result.training_loss),\n",
    "    'val_loss': float(val_results['eval_loss']),\n",
    "    'test_loss': float(test_results['eval_loss']),\n",
    "    'test_perplexity': float(np.exp(test_results['eval_loss'])),\n",
    "    'epochs': training_args.num_train_epochs,\n",
    "    'batch_size': training_args.per_device_train_batch_size,\n",
    "    'learning_rate': training_args.learning_rate,\n",
    "    'num_parameters': sum(p.numel() for p in model.parameters()),\n",
    "}\n",
    "\n",
    "with open(FINAL_MODEL_DIR / 'training_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Model saved successfully!\")\n",
    "print(f\"  Location: {FINAL_MODEL_DIR}\")\n",
    "print(f\"  Files: model weights, config, tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0263f",
   "metadata": {},
   "source": [
    "## 9. Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3106ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the trained model\n",
    "print(f\"Loading trained model from {FINAL_MODEL_DIR}...\")\n",
    "trained_model = AutoModelForMaskedLM.from_pretrained(str(FINAL_MODEL_DIR)).to(DEVICE)\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(str(FINAL_MODEL_DIR))\n",
    "\n",
    "# Create fill-mask pipeline\n",
    "fill_mask = pipeline(\"fill-mask\", model=trained_model, tokenizer=trained_tokenizer)\n",
    "\n",
    "print(\"âœ… Model loaded successfully!\\n\")\n",
    "\n",
    "# Test on Tamil text\n",
    "tamil_texts = [\n",
    "    \"à®¤à®®à®¿à®´à¯ à®®à¯Šà®´à®¿ [MASK] à®ªà®´à®®à¯ˆà®¯à®¾à®© à®®à¯Šà®´à®¿à®•à®³à®¿à®²à¯ à®’à®©à¯à®±à¯.\",\n",
    "    \"à®…à®°à®šà¯ [MASK] à®¨à®¿à®±à¯à®µà®©à®™à¯à®•à®³à¯ à®¨à®¾à®Ÿà¯à®Ÿà®¿à®©à¯ à®µà®³à®°à¯à®šà¯à®šà®¿à®•à¯à®•à¯ à®®à¯à®•à¯à®•à®¿à®¯à®®à¯.\",\n",
    "    \"à®¨à®²à¯à®² [MASK] à®µà®¾à®´à¯à®•à¯à®•à¯ˆà®¯à®¿à®©à¯ à®…à®Ÿà®¿à®ªà¯à®ªà®Ÿà¯ˆ à®†à®•à¯à®®à¯.\",\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Testing Aadhan Model:\\n\")\n",
    "for text in tamil_texts:\n",
    "    print(f\"Input: {text}\")\n",
    "    predictions = fill_mask(text)\n",
    "    print(f\"\\nTop 5 Predictions:\")\n",
    "    for i, pred in enumerate(predictions[:5], 1):\n",
    "        print(f\"  {i}. {pred['token_str']:<20} (Score: {pred['score']:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e805bc",
   "metadata": {},
   "source": [
    "## 10. Training Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef6044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics from logs\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Read training events\n",
    "logs_dir = LOGS_DIR\n",
    "trainer_state_file = CHECKPOINTS_DIR / 'trainer_state.json'\n",
    "\n",
    "if trainer_state_file.exists():\n",
    "    with open(trainer_state_file) as f:\n",
    "        trainer_state = json.load(f)\n",
    "    \n",
    "    # Extract metrics\n",
    "    epochs = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for log in trainer_state.get('log_history', []):\n",
    "        if 'loss' in log:\n",
    "            train_losses.append(log['loss'])\n",
    "        if 'eval_loss' in log:\n",
    "            val_losses.append(log['eval_loss'])\n",
    "            epochs.append(log.get('epoch', len(epochs) + 1))\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Training loss\n",
    "    ax = axes[0]\n",
    "    ax.plot(train_losses, marker='o', linestyle='-', label='Training Loss', color='blue')\n",
    "    ax.set_xlabel('Batch Step')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Loss Over Steps', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Validation loss per epoch\n",
    "    ax = axes[1]\n",
    "    ax.plot(epochs, val_losses, marker='s', linestyle='-', color='red', label='Validation Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Validation Loss Per Epoch', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(NOTEBOOKS_DIR / '02_training_metrics.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ… Metrics saved to {NOTEBOOKS_DIR / '02_training_metrics.png'}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Trainer state file not found. Skipping metrics visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a099251c",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### âœ… Completed in This Notebook\n",
    "\n",
    "- Loaded pre-tokenized Tamil corpus datasets\n",
    "- Loaded XLM-RoBERTa-base pretrained model (124M parameters)\n",
    "- Configured masked language modeling (MLM) with 15% masking probability\n",
    "- Set up training with optimized hyperparameters:\n",
    "  - Learning rate: 5e-5\n",
    "  - Batch size: 32 per GPU\n",
    "  - Warmup steps: 100\n",
    "  - Epochs: 10\n",
    "- Trained the model on modern Tamil corpus\n",
    "- Evaluated on validation and test sets\n",
    "- Saved trained model to `models/aadhan-mlm-v1/`\n",
    "- Tested model with fill-mask predictions\n",
    "- Generated training metrics visualization\n",
    "\n",
    "### ğŸ“Š Training Results\n",
    "\n",
    "- **Test Loss**: {:.4f}\n",
    "- **Test Perplexity**: {:.2f}\n",
    "- **Model Parameters**: 124M\n",
    "- **Training Device**: GPU/CPU\n",
    "\n",
    "### â­ï¸ Next Steps\n",
    "\n",
    "1. **Fine-tune for Downstream Tasks**\n",
    "   - Named Entity Recognition (NER)\n",
    "   - Sentiment Analysis\n",
    "   - Question Answering\n",
    "   - Text Classification\n",
    "\n",
    "2. **Evaluate on Tamil Benchmarks**\n",
    "   - Compare against other Tamil LLMs\n",
    "   - Measure performance on standard datasets\n",
    "\n",
    "3. **Convert to ONNX/Quantize**\n",
    "   - Deploy on edge devices\n",
    "   - Reduce model size for production\n",
    "\n",
    "4. **Build Instruction-Tuning Dataset**\n",
    "   - Create conversation pairs\n",
    "   - Human annotation for preference\n",
    "   - Train instruction-following model\n",
    "\n",
    "### ğŸ“ Model Location\n",
    "\n",
    "```\n",
    "models/aadhan-mlm-v1/\n",
    "â”œâ”€â”€ pytorch_model.bin      (Model weights)\n",
    "â”œâ”€â”€ config.json            (Model config)\n",
    "â”œâ”€â”€ tokenizer.json         (Tokenizer)\n",
    "â”œâ”€â”€ training_results.json  (Results summary)\n",
    "â””â”€â”€ README.md              (Model card)\n",
    "```\n",
    "\n",
    "### ğŸš€ Using the Trained Model\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
    "\n",
    "model_path = \"models/aadhan-mlm-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
    "\n",
    "# Use for fill-mask\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "results = fill_mask(\"à®¤à®®à®¿à®´à¯ à®®à¯Šà®´à®¿ [MASK] à®ªà®´à®®à¯ˆà®¯à®¾à®© à®®à¯Šà®´à®¿à®•à®³à®¿à®²à¯ à®’à®©à¯à®±à¯.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Status**: âœ… Model trained and saved  \n",
    "**Next Notebook**: `03_fine_tuning_tasks.ipynb` (optional, for downstream tasks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
